import json
import os
from langchain_core.documents import Document
from core.db import get_vector_store

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_PATH = os.path.join(os.path.dirname(__file__), "data.json")


def ingest_data():
    print(f"ğŸš€ å¼€å§‹åŠ è½½æ•°æ®: {DATA_PATH}")

    # 1. è¯»å– JSON
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    # 2. è½¬æ¢ä¸º LangChain Document å¯¹è±¡
    documents = []
    for item in data:
        # å°† similar_questions åˆå¹¶åˆ° content ä¸­ï¼Œå¢åŠ è¢«å¬å›çš„æ¦‚ç‡
        enhanced_content = f"{item['content']}\n\nç›¸å…³é—®é¢˜å‚è€ƒ:\n" + "\n".join(item['similar_questions'])

        doc = Document(
            page_content=enhanced_content,
            metadata={
                "id": item["id"],
                "category": item["category"],
                "topic": item["topic"],
                "source": item["source"]
            }
        )
        documents.append(doc)

    print(f"ğŸ“„ è§£æå®Œæˆï¼Œå…± {len(documents)} æ¡æ–‡æ¡£ã€‚æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å…¥ Chroma...")

    # 3. è·å–å‘é‡åº“è¿æ¥
    vector_store = get_vector_store()

    # 4. å­˜å…¥æ•°æ® (add_documents ä¼šè‡ªåŠ¨è°ƒç”¨ OpenAI Embedding API)
    # ids ç¡®ä¿å¦‚æœé‡å¤è¿è¡Œï¼Œå¯ä»¥é€šè¿‡ ID å»é‡æˆ–æ›´æ–°ï¼ˆå–å†³äºå…·ä½“å®ç°ï¼ŒChromaé€šå¸¸éœ€è¦æ‰‹åŠ¨å¤„ç†å»é‡ï¼Œè¿™é‡Œå…ˆç®€åŒ–ç›´æ¥æ·»åŠ ï¼‰
    ids = [d.metadata["id"] for d in documents]
    vector_store.add_documents(documents=documents, ids=ids)

    print("âœ… æ•°æ®å…¥åº“æˆåŠŸï¼")


if __name__ == "__main__":
    ingest_data()