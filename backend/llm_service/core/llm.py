import httpx
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import RedisChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from config import settings

# 1. åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    api_key=settings.OPENAI_API_KEY,
    base_url=settings.OPENAI_BASE_URL,
    model=settings.LLM_MODEL_NAME,
    temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œè®©å®ƒæ›´æ­»æ¿ã€æ›´å¬è¯
    streaming=True
)

# 2. è¿™é‡Œçš„ Prompt æ¨¡æ¿å†™å¾—è¶Šä¸¥å‰è¶Šå¥½
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªâ€œå¤§æ¨¡å‹åº”ç”¨å¼€å‘â€é¢†åŸŸçš„ä¸“å±æ™ºèƒ½å®¢æœã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. ã€ç»å¯¹ç¦æ­¢ã€‘ä½¿ç”¨ä½ é¢„è®­ç»ƒçš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚
2. ä½ åªèƒ½æ ¹æ®ä¸‹æ–¹çš„ã€æ£€ç´¢çŸ¥è¯†ã€‘æ¥å›ç­”é—®é¢˜ã€‚
3. å¦‚æœã€æ£€ç´¢çŸ¥è¯†ã€‘ä¸ºç©ºï¼Œæˆ–è€…ä¸é—®é¢˜æ— å…³ï¼Œä½ å¿…é¡»å›ç­”ï¼šâ€œæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ã€‚â€
4. è¯·ä¿æŒä¸“ä¸šã€ç®€æ´çš„è¯­æ°”ã€‚
5.ä½ æ˜¯ä¸€ä¸ªâ€œå¤§æ¨¡å‹åº”ç”¨å¼€å‘â€é¢†åŸŸçš„ä¸“å±æ™ºèƒ½å®¢æœ

ã€æ£€ç´¢çŸ¥è¯†ã€‘ï¼š
{context}"""),

    # æ’å…¥å†å²è®°å½•
    MessagesPlaceholder(variable_name="history"),

    ("human", "{question}"),
])


# 3. Redis History
def get_message_history(session_id: str):
    return RedisChatMessageHistory(
        session_id=session_id,
        url=settings.REDIS_URL,
        ttl=settings.SESSION_TTL
    )


async def search_knowledge_base(query: str):
    """
    è°ƒç”¨ KB Service è·å–ç›¸å…³çŸ¥è¯†
    """
    try:
        print(f"ğŸ” [DEBUG] æ­£åœ¨æ£€ç´¢: {query}")  # è°ƒè¯•æ—¥å¿—
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{settings.KB_SERVICE_URL}/documents/search",
                json={"query": query, "top_k": 3},
                timeout=10.0,
                headers={"X-Internal-Key": settings.INTERNAL_API_KEY}
            )

            if response.status_code == 200:
                data = response.json()
                results = data.get("results", [])

                # æ‰“å°æ£€ç´¢ç»“æœé•¿åº¦
                print(f"âœ… [DEBUG] æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡æ–‡æ¡£")

                context = "\n\n".join([f"æ–‡æ¡£{i + 1}: {item['content']}" for i, item in enumerate(results)])
                return context if context else ""  # å¦‚æœæ²¡ç»“æœï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            else:
                print(f"âŒ [DEBUG] KB Service æŠ¥é”™: {response.status_code} - {response.text}")
                return ""  # å‡ºé”™æ—¶è¿”å›ç©ºï¼Œé˜²æ­¢æ¨¡å‹è¯»åˆ°é”™è¯¯ä¿¡æ¯
    except Exception as e:
        print(f"âŒ [DEBUG] è¿æ¥ KB Service å¤±è´¥: {e}")
        return ""


async def rag_chat_stream(query: str, session_id: str):
    # 1. æ£€ç´¢
    context = await search_knowledge_base(query)

    # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®è°ƒè¯•ï¼šçœ‹çœ‹åˆ°åº•å‘ç»™äº†æ¨¡å‹ä»€ä¹ˆä¸Šä¸‹æ–‡ ğŸ”¥ğŸ”¥ğŸ”¥
    print(f"ğŸ“ [DEBUG] æœ€ç»ˆ Context å†…å®¹:\n{context}")
    print("--------------------------------------------------")

    # 2. æ„å»º Chain
    chain = prompt_template | llm | StrOutputParser()

    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_message_history,
        input_messages_key="question",
        history_messages_key="history",
    )

    # 3. æµå¼è°ƒç”¨
    async for chunk in chain_with_history.astream(
            {"question": query, "context": context},
            config={"configurable": {"session_id": session_id}}
    ):
        yield chunk